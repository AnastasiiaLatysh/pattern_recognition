У програмі прихований шар створюється:
model.add(Conv2D(filters=32, kernel_size=(3, 3), input_shape=(3, 32, 32), activation='relu', padding='same'))


Convolution (згортка) - це перший шар, який витягує features із вхідного зображення.
Згортка зберігає зв’язок між пікселями, вивчаючи особливості зображення,
використовуючи невеликі квадрати вхідних даних. Це математична операція, яка займає два входи,
такі як матриця зображення та фільтр або ядро.

Згортка зображення з різними фільтрами виконує такі операції, як виявлення країв, розмиття та різкість,
застосовуючи фільтри.
Іноді фільтр не підходить ідеально для вхідного зображення. Для цього ми використовуємо у нашій згортці padding="same".
"same" має такий результат, що вихідні дані мають таку ж довжину, що і вхідні.
"same" намагається прокладати рівномірно ліворуч та праворуч дані, але якщо кількість стовпців, які потрібно додати,
непарна, він додасть додатковий стовпець праворуч. Така ж логіка застосовується вертикально:
може бути додатковий рядок нулів внизу.

По-перше, нам потрібно переробити дані, придатні для тренування CNN.
У Keras шари, які використовуються для двовимірних згортків,
очікують значення пікселів із розмірами [канали] [ширина] [висота].
У випадку RGB, перший канал вимірювання буде 3 для трьох каналів: червоний, зелений і синій.

Convolution layer: шар згортки, який називається Conv2D:
• у ньому є 32 функції-детектора (тобто фільтри), розміром 3×3 (фільтри - це скільки у вас різних вікон.
Усі вони однакової довжини, яка є kernel_size, тобто це скільки різних результатів або каналів ви хочете створити.
Якщо ви використовуєте фільтри=32 і kernel_size = (3, 3), ви створюєте 32 різних фільтрів, кожен з яких має довжину (3, 3).
Результат принесе 32 різні згортки.
• ReLU як функція активації

ReLU (Rectified Linear Unit) використовуються для нелінійної операції.
Вихід ƒ(x) = max(0, x).
Мета ReLU полягає у впровадженні нелінійності у ConvNet.
Оскільки дані реального світу потребують, щоб ConvNet навчався на даних, які були б негативними лінійними значеннями.